# Chocolate-bar-rating_Machine_Learning
Report: Analysis and Modeling of Chocolate Bar Ratings
Introduction
The purpose of this report is to provide a summary and analysis of the findings obtained from the analysis and modeling of the chocolate bar ratings dataset. The report will cover the loading, initial insights, and data cleaning process, data analysis results, feature engineering techniques applied, categorical encoding methods used, modeling and evaluation process, hyperparameter tuning, dealing with classes within features, and a conclusion summarizing the key findings and potential areas for improvement.
1	Loading, Initial Insights, and Data Cleaning
The dataset, consisting of 1795 rows, was examined for missing data in the "Bean Type" and "Broad Bean Origin" features. Instead of removing these features due to the high percentage of missing data, a new class called "Unknown" was created to account for the missing values. No duplicates were found in the dataset, and the feature percentage was converted to a float number and had it % symbol removed.
2	Data Analysis
The distribution of ratings and cocoa percentages followed a Gaussian profile with a tail towards low ratings and high cocoa percentages. Chocolates with around 70% cocoa were associated with higher ratings. Although ratings did not consistently increase over the years, concentrations of ratings were observed around 2014 and 2016. Chocolates from the USA received the highest ratings, and those with 70% and 72% cocoa content were generally rated highly. Chocolates made from Trinitario and Criollo beans also received high ratings. Outliers analysis revealed interesting insights, such as the high ratings (>4 ) for Amedei chocolates, while only three chocolates had their ratings around one, and only two above rating four. A Pearson correlation matrix was computed, where I identified that two features (REF and Review_Date) were very much correlated.
3	Feature Engineering
Feature engineering techniques were applied to extract more meaningful information from the "REF" and "Review_Date" features, which exhibited a high correlation coefficient of 0.99, indicating redundancy. New features, such as the year from the "Review_Date" and the time difference between the review date and a reference date, were created to provide valuable insights for the model.
4	Categorical Encoding
Given that most of the features had categorical variables, a categorical encoding was performed to prepare the data for modeling. Target encoding, specifically mean encoding, was chosen over one-hot encoding due to the large number of classes in some features. To address target leakage, a smoothing term was included in the target encoding process.
5	Modeling and Evaluation
XGBoost, a powerful and versatile algorithm, was selected for modeling due to its speed and performance. Evaluation metrics such as SMAPE, MAE, RMSE, and R2 score were used to assess the model's performance. The model performed reasonably well on the training set, achieving an R2 score of 0.87. However, the performance on the test set was slightly lower, with an R2 score of 0.62, suggesting potential overfitting.
Hyperparameter tuning was performed to optimize the model's performance. The best model was selected based on the closest R2 values for both the training and test datasets. The tuned model improved performance, reducing overfitting and achieving satisfactory results. An analysis of Feature importance was computed, which indicated that two features were quite important for the modelling, they were: "Specific Bean Origin or Bar Name" and "Company" (Please see Cell 70).
6	Dealing with Classes within Features; and
7	Re-modelling
I considered that a way to improve the model, specifically on the test dataset was to reduce the number of classes present in the two features aforementioned. To address the large number of classes in the "Specific Bean Origin or Bar Name" and "Company" features, grouping techniques were applied. However, these modifications did not improve the model's performance and resulted in worse evaluation metrics. Therefore, I considered that the previous models were still performing better and reproducing more accurately the test dataset.
8	Conclusion
In conclusion, the analysis and modeling of the chocolate bar ratings dataset provided valuable insights into the factors influencing ratings. Feature engineering, categorical encoding, hyperparameter tuning and model ensemble were performed to improve the model's performance. The selected XGBoost model achieved satisfactory results but showed potential for overfitting due to the limited amount of data. Further improvements could be explored, such as reducing the number of classes in high-dimensional features or increase the dataset by means of synthetic data creation, but both of them would require more than and deeper data exploration.
9	Appendix: Extra Tests
Additional tests, including rounding the ratings to specific values, were performed but did not lead to significant improvements in the model's performance.
Overall, this project provided insights into the chocolate bar ratings dataset and demonstrated the steps involved in data analysis, feature engineering, modeling, and evaluation. While the model achieved satisfactory results, further improvements and exploration can be done to enhance the model's performance.


